# %% [markdown]
# # CORD-19 Dataset Exploratory Analysis
# 
# This notebook provides a step-by-step exploration of the CORD-19 dataset, focusing on understanding the structure, quality, and patterns in COVID-19 research papers.

# %% [markdown]
# ## 1. Setup and Data Loading

# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
import re
from collections import Counter
from wordcloud import WordCloud

# Set display options
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
warnings.filterwarnings('ignore')

# Set plotting style
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

print("Libraries imported successfully!")

# %%
# Load the dataset
# Note: Make sure you have downloaded metadata.csv from CORD-19 dataset
try:
    df = pd.read_csv('metadata.csv')
    print(f"âœ… Dataset loaded successfully!")
    print(f"ğŸ“Š Shape: {df.shape[0]:,} rows, {df.shape[1]} columns")
except FileNotFoundError:
    print("âŒ metadata.csv not found. Please download it from CORD-19 dataset.")
    print("URL: https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge")

# %% [markdown]
# ## 2. Initial Data Exploration

# %%
# Display basic information about the dataset
print("=== DATASET OVERVIEW ===")
print(f"Shape: {df.shape}")
print(f"\nColumn names:")
for i, col in enumerate(df.columns, 1):
    print(f"{i:2d}. {col}")

# %%
# Check data types and basic info
df.info()

# %%
# Display first few rows
print("=== FIRST 5 ROWS ===")
df.head()

# %%
# Basic statistics for numerical columns
print("=== BASIC STATISTICS ===")
df.describe()

# %% [markdown]
# ## 3. Missing Values Analysis

# %%
# Calculate missing values
missing_data = df.isnull().sum()
missing_percent = (missing_data / len(df)) * 100

missing_df = pd.DataFrame({
    'Column': missing_data.index,
    'Missing_Count': missing_data.values,
    'Missing_Percentage': missing_percent.values
}).sort_values('Missing_Percentage', ascending=False)

print("=== MISSING VALUES ANALYSIS ===")
print(missing_df[missing_df['Missing_Count'] > 0])

# %%
# Visualize missing values
plt.figure(figsize=(12, 8))
missing_cols = missing_df[missing_df['Missing_Count'] > 0]

if len(missing_cols) > 0:
    bars = plt.barh(missing_cols['Column'], missing_cols['Missing_Percentage'])
    plt.title('Missing Values by Column (%)', fontsize=16, fontweight='bold')
    plt.xlabel('Missing Percentage (%)')
    plt.ylabel('Columns')
    
    # Add percentage labels on bars
    for i, bar in enumerate(bars):
        width = bar.get_width()
        plt.text(width + 0.5, bar.get_y() + bar.get_height()/2, 
                f'{width:.1f}%', ha='left', va='center')
    
    plt.tight_layout()
    plt.show()
else:
    print("No missing values found!")

# %% [markdown]
# ## 4. Data Cleaning

# %%
# Create a cleaned version of the dataset
df_clean = df.copy()
original_shape = df_clean.shape

print("=== DATA CLEANING PROCESS ===")
print(f"Original shape: {original_shape}")

# Convert publish_time to datetime
if 'publish_time' in df_clean.columns:
    df_clean['publish_time'] = pd.to_datetime(df_clean['publish_time'], errors='coerce')
    df_clean['year'] = df_clean['publish_time'].dt.year
    df_clean['month'] = df_clean['publish_time'].dt.month
    print("âœ… Date columns processed")

# Remove rows without title
if 'title' in df_clean.columns:
    before_title_clean = len(df_clean)
    df_clean = df_clean.dropna(subset=['title'])
    print(f"âœ… Removed {before_title_clean - len(df_clean):,} rows without titles")

# Fill missing journal names
if 'journal' in df_clean.columns:
    df_clean['journal'] = df_clean['journal'].fillna('Unknown Journal')
    print("âœ… Filled missing journal names")

# Create word count features
if 'abstract' in df_clean.columns:
    df_clean['abstract_word_count'] = df_clean['abstract'].fillna('').apply(
        lambda x: len(str(x).split()) if pd.notna(x) else 0
    )
    print("âœ… Added abstract word count")

if 'title' in df_clean.columns:
    df_clean['title_word_count'] = df_clean['title'].apply(
        lambda x: len(str(x).split()) if pd.notna(x) else 0
    )
    print("âœ… Added title word count")

print(f"Final shape: {df_clean.shape}")
print(f"Rows removed: {original_shape[0] - df_clean.shape[0]:,}")

# %% [markdown]
# ## 5. Publication Trends Analysis

# %%
# Analyze publications by year
if 'year' in df_clean.columns:
    # Filter reasonable years (2000-2023)
    year_data = df_clean['year'][(df_clean['year'] >= 2000) & (df_clean['year'] <= 2023)]
    year_counts = year_data.value_counts().sort_index()
    
    plt.figure(figsize=(14, 8))
    bars = plt.bar(year_counts.index, year_counts.values, color='skyblue', alpha=0.8)
    plt.title('COVID-19 Research Publications by Year', fontsize=18, fontweight='bold')
    plt.xlabel('Year', fontsize=14)
    plt.ylabel('Number of Publications', fontsize=14)
    plt.xticks(rotation=45)
    plt.grid(axis='y', alpha=0.3)
    
    # Add value labels on bars
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 50,
                f'{int(height):,}', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.show()
    
    # Print insights
    print("=== PUBLICATION TRENDS INSIGHTS ===")
    print(f"Total years covered: {len(year_counts)}")
    print(f"Peak year: {year_counts.idxmax()} with {year_counts.max():,} papers")
    print(f"Most recent year: {year_counts.index.max()}")
    print(f"Growth from 2019 to 2020: {((year_counts.get(2020, 0) - year_counts.get(2019, 0)) / year_counts.get(2019, 1) * 100):+.1f}%")

# %% [markdown]
# ## 6. Journal Analysis

# %%
# Analyze top journals
if 'journal' in df_clean.columns:
    top_journals = 20
    journal_counts = df_clean['journal'].value_counts().head(top_journals)
    
    plt.figure(figsize=(14, 10))
    bars = plt.barh(range(len(journal_counts)), journal_counts.values, color='lightcoral', alpha=0.8)
    plt.title(f'Top {top_journals} Journals Publishing COVID-19 Research', fontsize=18, fontweight='bold')
    plt.xlabel('Number of Publications', fontsize=14)
    plt.ylabel('Journals', fontsize=14)
    plt.yticks(range(len(journal_counts)), journal_counts.index)
    
    # Add value labels
    for i, bar in enumerate(bars):
        width = bar.get_width()
        plt.text(width + 10, bar.get_y() + bar.get_height()/2,
                f'{int(width):,}', va='center', fontweight='bold')
    
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()
    
    # Print journal statistics
    print("=== JOURNAL ANALYSIS INSIGHTS ===")
    print(f"Total unique journals: {df_clean['journal'].nunique():,}")
    print(f"Top journal: {journal_counts.index[0]} ({journal_counts.iloc[0]:,} papers)")
    print(f"Top {top_journals} journals account for {journal_counts.sum():,} papers ({journal_counts.sum()/len(df_clean)*100:.1f}%)")

# %% [markdown]
# ## 7. Word Analysis from Titles

# %%
# Analyze most frequent words in titles
if 'title' in df_clean.columns:
    # Combine all titles
    all_titles = ' '.join(df_clean['title'].fillna('').astype(str))
    
    # Clean and extract words
    words = re.findall(r'\b[a-zA-Z]{3,}\b', all_titles.lower())
    
    # Remove common stop words
    stop_words = {
        'the', 'and', 'for', 'are', 'with', 'this', 'that', 'from', 'they', 'been', 
        'have', 'were', 'said', 'each', 'which', 'their', 'time', 'will', 'about', 
        'would', 'there', 'could', 'other', 'more', 'very', 'what', 'know', 'just', 
        'first', 'into', 'over', 'think', 'also', 'your', 'work', 'life', 'only', 
        'can', 'still', 'should', 'after', 'being', 'now', 'made', 'before', 'here', 
        'through', 'when', 'where', 'much', 'some', 'has', 'had', 'did', 'get', 
        'may', 'him', 'old', 'see', 'two', 'way', 'who', 'its', 'make', 'most', 
        'her', 'use', 'day', 'even', 'new', 'want', 'because', 'any', 'these', 
        'give', 'man', 'our', 'under', 'never'
    }
    
    filtered_words = [word for word in words if word not in stop_words]
    word_freq = Counter(filtered_words)
    top_words = word_freq.most_common(25)
    
    if top_words:
        words_list, counts_list = zip(*top_words)
        
        plt.figure(figsize=(14, 10))
        bars = plt.barh(range(len(words_list)), counts_list, color='lightgreen', alpha=0.8)
        plt.title('Top 25 Words in Paper Titles', fontsize=18, fontweight='bold')
        plt.xlabel('Frequency', fontsize=14)
        plt.ylabel('Words', fontsize=14)
        plt.yticks(range(len(words_list)), words_list)
        
        # Add frequency labels
        for i, bar in enumerate(bars):
            width = bar.get_width()
            plt.text(width + 50, bar.get_y() + bar.get_height()/2,
                    f'{int(width):,}', va='center', fontweight='bold')
        
        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.show()

# %%
# Create word cloud
if 'title' in df_clean.columns:
    # Create word cloud from titles
    wordcloud = WordCloud(
        width=1200, height=600,
        background_color='white',
        max_words=150,
        colormap='viridis',
        relative_scaling=0.5
    ).generate(all_titles)
    
    plt.figure(figsize=(16, 8))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title('Word Cloud of Paper Titles', fontsize=20, fontweight='bold', pad=20)
    plt.axis('off')
    plt.tight_layout()
    plt.show()

# %% [markdown]
# ## 8. Abstract Analysis

# %%
# Analyze abstract characteristics
if 'abstract_word_count' in df_clean.columns:
    # Remove zero word count abstracts
    abstract_lengths = df_clean[df_clean['abstract_word_count'] > 0]['abstract_word_count']
    
    plt.figure(figsize=(14, 6))
    plt.subplot(1, 2, 1)
    plt.hist(abstract_lengths, bins=50, alpha=0.7, color='orange')
    plt.title('Distribution of Abstract Lengths', fontweight='bold')
    plt.xlabel('Word Count')
    plt.ylabel('Frequency')
    plt.grid(alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.boxplot(abstract_lengths)
    plt.title('Abstract Length Box Plot', fontweight='bold')
    plt.ylabel('Word Count')
    plt.grid(alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print("=== ABSTRACT ANALYSIS ===")
    print(f"Papers with abstracts: {len(abstract_lengths):,}")
    print(f"Average abstract length: {abstract_lengths.mean():.1f} words")
    print(f"Median abstract length: {abstract_lengths.median():.1f} words")
    print(f"Shortest abstract: {abstract_lengths.min()} words")
    print(f"Longest abstract: {abstract_lengths.max()} words")

# %% [markdown]
# ## 9. Source Analysis

# %%
# Analyze papers by source
source_col = None
for col in ['source_x', 'source']:
    if col in df_clean.columns:
        source_col = col
        break

if source_col:
    source_counts = df_clean[source_col].value_counts()
    
    plt.figure(figsize=(12, 8))
    colors = plt.cm.Set3(np.linspace(0, 1, len(source_counts)))
    wedges, texts, autotexts = plt.pie(source_counts.values, labels=source_counts.index, 
                                      autopct='%1.1f%%', startangle=90, colors=colors)
    plt.title('Distribution of Papers by Source', fontsize=18, fontweight='bold')
    
    # Enhance text appearance
    for autotext in autotexts:
        autotext.set_color('white')
        autotext.set_fontweight('bold')
    
    plt.axis('equal')
    plt.tight_layout()
    plt.show()
    
    print("=== SOURCE DISTRIBUTION ===")
    for source, count in source_counts.items():
        percentage = (count / len(df_clean)) * 100
        print(f"{source}: {count:,} papers ({percentage:.1f}%)")

# %% [markdown]
# ## 10. Correlation Analysis

# %%
# Analyze correlations between numerical features
numerical_cols = df_clean.select_dtypes(include=[np.number]).columns
if len(numerical_cols) > 1:
    correlation_matrix = df_clean[numerical_cols].corr()
    
    plt.figure(figsize=(12, 10))
    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', 
                center=0, square=True, linewidths=.5, cbar_kws={"shrink": .5})
    plt.title('Correlation Matrix of Numerical Features', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()

# %% [markdown]
# ## 11. Summary and Key Insights

# %%
print("\n" + "="*70)
print("CORD-19 DATASET ANALYSIS SUMMARY")
print("="*70)

print(f"\nğŸ“Š DATASET OVERVIEW:")
print(f"   â€¢ Total papers analyzed: {len(df_clean):,}")
print(f"   â€¢ Original dataset size: {len(df):,}")
print(f"   â€¢ Data cleaning removed: {len(df) - len(df_clean):,} rows")

if 'year' in df_clean.columns:
    print(f"\nğŸ“… TEMPORAL ANALYSIS:")
    print(f"   â€¢ Year range: {df_clean['year'].min():.0f} - {df_clean['year'].max():.0f}")
    peak_year = df_clean['year'].value_counts().index[0]
    peak_count = df_clean['year'].value_counts().iloc[0]
    print(f"   â€¢ Peak publication year: {peak_year} ({peak_count:,} papers)")

if 'journal' in df_clean.columns:
    print(f"\nğŸ“° JOURNAL ANALYSIS:")
    print(f"   â€¢ Unique journals: {df_clean['journal'].nunique():,}")
    top_journal = df_clean['journal'].value_counts().index[0]
    top_count = df_clean['journal'].value_counts().iloc[0]
    print(f"   â€¢ Most prolific journal: {top_journal} ({top_count:,} papers)")

if 'abstract_word_count' in df_clean.columns:
    avg_abstract = df_clean[df_clean['abstract_word_count'] > 0]['abstract_word_count'].mean()
    print(f"\nğŸ“ CONTENT ANALYSIS:")
    print(f"   â€¢ Average abstract length: {avg_abstract:.1f} words")
    papers_with_abstracts = (df_clean['abstract_word_count'] > 0).sum()
    print(f"   â€¢ Papers with abstracts: {papers_with_abstracts:,} ({papers_with_abstracts/len(df_clean)*100:.1f}%)")

print(f"\nğŸ” KEY INSIGHTS:")
print("   â€¢ COVID-19 research shows explosive growth during 2020-2021")
print("   â€¢ Medical and public health journals dominate the publication landscape")
print("   â€¢ Research terms focus heavily on clinical and epidemiological aspects")
print("   â€¢ Multi-source dataset provides comprehensive coverage of research")
print("   â€¢ Abstract availability varies significantly across sources")

print("\n" + "="*70)
print("Analysis completed! ğŸ‰")
print("="*70)

# %% [markdown]
# ## 12. Export Cleaned Data 

# %%

print("Saving cleaned dataset...")
df_clean.to_csv('cord19_cleaned.csv', index=False)
print("âœ… Cleaned dataset saved as 'cord19_cleaned.csv'")
